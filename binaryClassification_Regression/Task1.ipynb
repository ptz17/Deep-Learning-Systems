{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration, Loss = 54.73064318447574, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "1-th iteration, Loss = 54.67940307237551, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "2-th iteration, Loss = 54.62190182247811, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "3-th iteration, Loss = 54.556989229945756, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "4-th iteration, Loss = 54.48321943862738, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "5-th iteration, Loss = 54.39875449277129, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "6-th iteration, Loss = 54.30123198642019, \tTrain acc = 0.5125, \tTest acc = 0.6\n",
      "7-th iteration, Loss = 54.1875836253343, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "8-th iteration, Loss = 54.05378881936778, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "9-th iteration, Loss = 53.89454816110343, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "10-th iteration, Loss = 53.70287350684662, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "11-th iteration, Loss = 53.46963189230769, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "12-th iteration, Loss = 53.18318358194855, \tTrain acc = 0.525, \tTest acc = 0.6\n",
      "13-th iteration, Loss = 52.82946680468626, \tTrain acc = 0.5375, \tTest acc = 0.6\n",
      "14-th iteration, Loss = 52.39320424349336, \tTrain acc = 0.5625, \tTest acc = 0.6\n",
      "15-th iteration, Loss = 51.86109209396144, \tTrain acc = 0.5875, \tTest acc = 0.6\n",
      "16-th iteration, Loss = 51.227097553881976, \tTrain acc = 0.6375, \tTest acc = 0.6\n",
      "17-th iteration, Loss = 50.49754979558279, \tTrain acc = 0.6625, \tTest acc = 0.6\n",
      "18-th iteration, Loss = 49.69147404916335, \tTrain acc = 0.675, \tTest acc = 0.7\n",
      "19-th iteration, Loss = 48.83472554879617, \tTrain acc = 0.7375, \tTest acc = 0.7\n",
      "20-th iteration, Loss = 47.95340286427566, \tTrain acc = 0.7625, \tTest acc = 0.7\n",
      "21-th iteration, Loss = 47.071844241774116, \tTrain acc = 0.8, \tTest acc = 0.7\n",
      "22-th iteration, Loss = 46.213678374223385, \tTrain acc = 0.8, \tTest acc = 0.7\n",
      "23-th iteration, Loss = 45.402304138704835, \tTrain acc = 0.825, \tTest acc = 0.7\n",
      "24-th iteration, Loss = 44.6599005983049, \tTrain acc = 0.8625, \tTest acc = 0.8\n",
      "25-th iteration, Loss = 44.0048839667262, \tTrain acc = 0.9125, \tTest acc = 0.9\n",
      "26-th iteration, Loss = 43.447714610471266, \tTrain acc = 0.95, \tTest acc = 0.95\n",
      "27-th iteration, Loss = 42.98777769786784, \tTrain acc = 0.9625, \tTest acc = 0.95\n",
      "28-th iteration, Loss = 42.6147887230702, \tTrain acc = 0.9625, \tTest acc = 0.95\n",
      "29-th iteration, Loss = 42.31372794564949, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "30-th iteration, Loss = 42.069483499748, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "31-th iteration, Loss = 41.869236584580904, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "32-th iteration, Loss = 41.702989713521546, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "33-th iteration, Loss = 41.56322996396297, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "34-th iteration, Loss = 41.44437261627773, \tTrain acc = 0.9625, \tTest acc = 1.0\n",
      "35-th iteration, Loss = 41.3422515387436, \tTrain acc = 0.975, \tTest acc = 1.0\n",
      "36-th iteration, Loss = 41.25372424311786, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "37-th iteration, Loss = 41.17638560700306, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "38-th iteration, Loss = 41.10836577869494, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "39-th iteration, Loss = 41.04818846406741, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "40-th iteration, Loss = 40.99467116761572, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "41-th iteration, Loss = 40.94685427032844, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "42-th iteration, Loss = 40.90394993440961, \tTrain acc = 0.9875, \tTest acc = 1.0\n",
      "43-th iteration, Loss = 40.865304731819776, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "44-th iteration, Loss = 40.83037187364082, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "45-th iteration, Loss = 40.798690244231416, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "46-th iteration, Loss = 40.76986832902985, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "47-th iteration, Loss = 40.74357171580756, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "48-th iteration, Loss = 40.719513245972934, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "49-th iteration, Loss = 40.69744516113873, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "50-th iteration, Loss = 40.67715277386135, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "51-th iteration, Loss = 40.65844931855394, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "52-th iteration, Loss = 40.641171727643545, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "53-th iteration, Loss = 40.62517714131725, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "54-th iteration, Loss = 40.610340004790956, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "55-th iteration, Loss = 40.59654964035194, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "56-th iteration, Loss = 40.5837082061192, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "57-th iteration, Loss = 40.57172897201693, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "58-th iteration, Loss = 40.560534857573785, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "59-th iteration, Loss = 40.55005718703331, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "60-th iteration, Loss = 40.54023462572762, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "61-th iteration, Loss = 40.53101226832557, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "62-th iteration, Loss = 40.522340854851606, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "63-th iteration, Loss = 40.51417609460013, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "64-th iteration, Loss = 40.506478081477425, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "65-th iteration, Loss = 40.499210787067184, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "66-th iteration, Loss = 40.49234161997016, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "67-th iteration, Loss = 40.4858410418167, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "68-th iteration, Loss = 40.479682231874264, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "69-th iteration, Loss = 40.47384079343158, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "70-th iteration, Loss = 40.46829449618731, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "71-th iteration, Loss = 40.463023049743455, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "72-th iteration, Loss = 40.45800790403226, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "73-th iteration, Loss = 40.45323207311797, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "74-th iteration, Loss = 40.44867997932815, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "75-th iteration, Loss = 40.444337315104335, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "76-th iteration, Loss = 40.440190920328035, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "77-th iteration, Loss = 40.436228673189675, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "78-th iteration, Loss = 40.43243939293265, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "79-th iteration, Loss = 40.42881275302943, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "80-th iteration, Loss = 40.425339203539636, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "81-th iteration, Loss = 40.42200990156407, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "82-th iteration, Loss = 40.418816648849926, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "83-th iteration, Loss = 40.41575183572367, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "84-th iteration, Loss = 40.41280839063239, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "85-th iteration, Loss = 40.40997973466423, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "86-th iteration, Loss = 40.40725974049659, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "87-th iteration, Loss = 40.40464269528778, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "88-th iteration, Loss = 40.402123267086516, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "89-th iteration, Loss = 40.39969647438403, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "90-th iteration, Loss = 40.397357658477986, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "91-th iteration, Loss = 40.39510245835584, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "92-th iteration, Loss = 40.39292678783867, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "93-th iteration, Loss = 40.39082681475638, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "94-th iteration, Loss = 40.38879894195027, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "95-th iteration, Loss = 40.38683978992229, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "96-th iteration, Loss = 40.38494618096957, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "97-th iteration, Loss = 40.38311512466066, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "98-th iteration, Loss = 40.38134380452519, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "99-th iteration, Loss = 40.37962956584227, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "100-th iteration, Loss = 40.37796990442499, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "101-th iteration, Loss = 40.37636245630905, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "102-th iteration, Loss = 40.37480498826294, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "103-th iteration, Loss = 40.373295389045474, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "104-th iteration, Loss = 40.37183166134406, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "105-th iteration, Loss = 40.37041191433345, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "106-th iteration, Loss = 40.36903435680085, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "107-th iteration, Loss = 40.3676972907885, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "108-th iteration, Loss = 40.36639910570936, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "109-th iteration, Loss = 40.36513827289613, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "110-th iteration, Loss = 40.363913340547136, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "111-th iteration, Loss = 40.362722929036245, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "112-th iteration, Loss = 40.36156572655719, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "113-th iteration, Loss = 40.36044048507495, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "114-th iteration, Loss = 40.359346016559684, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "115-th iteration, Loss = 40.35828118948074, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "116-th iteration, Loss = 40.35724492554023, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "117-th iteration, Loss = 40.35623619662765, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "118-th iteration, Loss = 40.35525402197841, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "119-th iteration, Loss = 40.35429746552067, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "120-th iteration, Loss = 40.35336563339642, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "121-th iteration, Loss = 40.35245767164356, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "122-th iteration, Loss = 40.351572764027196, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "123-th iteration, Loss = 40.350710130009105, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "124-th iteration, Loss = 40.34986902284537, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "125-th iteration, Loss = 40.34904872780295, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "126-th iteration, Loss = 40.34824856048667, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "127-th iteration, Loss = 40.347467865268754, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "128-th iteration, Loss = 40.34670601381396, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "129-th iteration, Loss = 40.34596240369333, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "130-th iteration, Loss = 40.34523645708077, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "131-th iteration, Loss = 40.34452761952659, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "132-th iteration, Loss = 40.34383535880295, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "133-th iteration, Loss = 40.34315916381624, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "134-th iteration, Loss = 40.34249854358224, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "135-th iteration, Loss = 40.34185302625946, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "136-th iteration, Loss = 40.34122215823733, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "137-th iteration, Loss = 40.340605503275455, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "138-th iteration, Loss = 40.34000264169059, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "139-th iteration, Loss = 40.33941316958857, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "140-th iteration, Loss = 40.33883669813804, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "141-th iteration, Loss = 40.33827285288358, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "142-th iteration, Loss = 40.33772127309575, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "143-th iteration, Loss = 40.337181611155756, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "144-th iteration, Loss = 40.33665353197249, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "145-th iteration, Loss = 40.3361367124303, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "146-th iteration, Loss = 40.33563084086538, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "147-th iteration, Loss = 40.33513561656904, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "148-th iteration, Loss = 40.3346507493166, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "149-th iteration, Loss = 40.334175958920014, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "150-th iteration, Loss = 40.333710974803054, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "151-th iteration, Loss = 40.333255535597715, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "152-th iteration, Loss = 40.332809388760666, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "153-th iteration, Loss = 40.33237229020844, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "154-th iteration, Loss = 40.33194400397058, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "155-th iteration, Loss = 40.33152430185942, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "156-th iteration, Loss = 40.331112963155874, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "157-th iteration, Loss = 40.33070977431009, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "158-th iteration, Loss = 40.330314528656395, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "159-th iteration, Loss = 40.32992702614153, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "160-th iteration, Loss = 40.3295470730656, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "161-th iteration, Loss = 40.32917448183505, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "162-th iteration, Loss = 40.32880907072691, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "163-th iteration, Loss = 40.32845066366396, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "164-th iteration, Loss = 40.32809908999984, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "165-th iteration, Loss = 40.32775418431406, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "166-th iteration, Loss = 40.32741578621604, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "167-th iteration, Loss = 40.32708374015785, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "168-th iteration, Loss = 40.326757895255255, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "169-th iteration, Loss = 40.32643810511651, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "170-th iteration, Loss = 40.32612422767865, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "171-th iteration, Loss = 40.32581612505082, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "172-th iteration, Loss = 40.325513663364305, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "173-th iteration, Loss = 40.32521671262899, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "174-th iteration, Loss = 40.324925146595824, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "175-th iteration, Loss = 40.32463884262507, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "176-th iteration, Loss = 40.324357681560116, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "177-th iteration, Loss = 40.32408154760643, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "178-th iteration, Loss = 40.32381032821555, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "179-th iteration, Loss = 40.32354391397378, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "180-th iteration, Loss = 40.323282198495434, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "181-th iteration, Loss = 40.32302507832041, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "182-th iteration, Loss = 40.322772452815784, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "183-th iteration, Loss = 40.32252422408146, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "184-th iteration, Loss = 40.322280296859404, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "185-th iteration, Loss = 40.32204057844658, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "186-th iteration, Loss = 40.32180497861124, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "187-th iteration, Loss = 40.3215734095124, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "188-th iteration, Loss = 40.32134578562255, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "189-th iteration, Loss = 40.32112202365325, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "190-th iteration, Loss = 40.320902042483574, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "191-th iteration, Loss = 40.32068576309128, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "192-th iteration, Loss = 40.32047310848655, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "193-th iteration, Loss = 40.32026400364829, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "194-th iteration, Loss = 40.32005837546265, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "195-th iteration, Loss = 40.31985615266396, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "196-th iteration, Loss = 40.31965726577782, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "197-th iteration, Loss = 40.31946164706616, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "198-th iteration, Loss = 40.31926923047445, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "199-th iteration, Loss = 40.31907995158069, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "200-th iteration, Loss = 40.31889374754634, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "201-th iteration, Loss = 40.3187105570689, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "202-th iteration, Loss = 40.31853032033624, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "203-th iteration, Loss = 40.31835297898254, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "204-th iteration, Loss = 40.31817847604578, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "205-th iteration, Loss = 40.318006755926646, \tTrain acc = 1.0, \tTest acc = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206-th iteration, Loss = 40.31783776434895, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "207-th iteration, Loss = 40.317671448321406, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "208-th iteration, Loss = 40.31750775610061, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "209-th iteration, Loss = 40.31734663715548, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "210-th iteration, Loss = 40.3171880421327, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "211-th iteration, Loss = 40.31703192282347, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "212-th iteration, Loss = 40.31687823213129, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "213-th iteration, Loss = 40.31672692404091, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "214-th iteration, Loss = 40.31657795358818, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "215-th iteration, Loss = 40.31643127683107, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "216-th iteration, Loss = 40.31628685082141, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "217-th iteration, Loss = 40.31614463357785, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "218-th iteration, Loss = 40.316004584059385, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "219-th iteration, Loss = 40.315866662139996, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "220-th iteration, Loss = 40.31573082858392, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "221-th iteration, Loss = 40.31559704502188, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "222-th iteration, Loss = 40.31546527392789, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "223-th iteration, Loss = 40.31533547859694, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "224-th iteration, Loss = 40.31520762312333, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "225-th iteration, Loss = 40.31508167237965, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "226-th iteration, Loss = 40.31495759199649, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "227-th iteration, Loss = 40.314835348342726, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "228-th iteration, Loss = 40.31471490850639, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "229-th iteration, Loss = 40.314596240276224, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "230-th iteration, Loss = 40.31447931212371, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "231-th iteration, Loss = 40.31436409318565, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "232-th iteration, Loss = 40.31425055324735, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "233-th iteration, Loss = 40.31413866272621, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "234-th iteration, Loss = 40.314028392655885, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "235-th iteration, Loss = 40.31391971467087, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "236-th iteration, Loss = 40.313812600991554, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "237-th iteration, Loss = 40.31370702440975, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "238-th iteration, Loss = 40.313602958274586, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "239-th iteration, Loss = 40.313500376478835, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "240-th iteration, Loss = 40.313399253445695, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "241-th iteration, Loss = 40.31329956411588, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "242-th iteration, Loss = 40.313201283935065, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "243-th iteration, Loss = 40.31310438884185, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "244-th iteration, Loss = 40.313008855255816, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "245-th iteration, Loss = 40.31291466006617, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "246-th iteration, Loss = 40.31282178062051, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "247-th iteration, Loss = 40.312730194714064, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "248-th iteration, Loss = 40.31263988057911, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "249-th iteration, Loss = 40.31255081687479, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "250-th iteration, Loss = 40.31246298267709, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "251-th iteration, Loss = 40.31237635746922, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "252-th iteration, Loss = 40.3122909211322, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "253-th iteration, Loss = 40.3122066539357, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "254-th iteration, Loss = 40.31212353652908, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "255-th iteration, Loss = 40.31204154993284, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "256-th iteration, Loss = 40.31196067553009, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "257-th iteration, Loss = 40.311880895058415, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "258-th iteration, Loss = 40.31180219060184, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "259-th iteration, Loss = 40.31172454458312, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "260-th iteration, Loss = 40.3116479397561, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "261-th iteration, Loss = 40.311572359198436, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "262-th iteration, Loss = 40.31149778630438, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "263-th iteration, Loss = 40.31142420477776, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "264-th iteration, Loss = 40.311351598625265, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "265-th iteration, Loss = 40.31127995214975, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "266-th iteration, Loss = 40.311209249943836, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "267-th iteration, Loss = 40.31113947688357, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "268-th iteration, Loss = 40.31107061812233, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "269-th iteration, Loss = 40.31100265908486, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "270-th iteration, Loss = 40.31093558546145, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "271-th iteration, Loss = 40.31086938320223, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "272-th iteration, Loss = 40.310804038511684, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "273-th iteration, Loss = 40.310739537843254, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "274-th iteration, Loss = 40.31067586789406, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "275-th iteration, Loss = 40.31061301559983, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "276-th iteration, Loss = 40.31055096812979, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "277-th iteration, Loss = 40.31048971288194, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "278-th iteration, Loss = 40.31042923747818, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "279-th iteration, Loss = 40.310369529759754, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "280-th iteration, Loss = 40.31031057778263, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "281-th iteration, Loss = 40.3102523698132, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "282-th iteration, Loss = 40.31019489432388, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "283-th iteration, Loss = 40.31013813998892, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "284-th iteration, Loss = 40.310082095680386, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "285-th iteration, Loss = 40.31002675046399, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "286-th iteration, Loss = 40.309972093595356, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "287-th iteration, Loss = 40.30991811451608, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "288-th iteration, Loss = 40.30986480285003, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "289-th iteration, Loss = 40.30981214839976, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "290-th iteration, Loss = 40.30976014114286, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "291-th iteration, Loss = 40.309708771228564, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "292-th iteration, Loss = 40.30965802897434, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "293-th iteration, Loss = 40.30960790486252, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "294-th iteration, Loss = 40.30955838953715, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "295-th iteration, Loss = 40.30950947380077, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "296-th iteration, Loss = 40.309461148611334, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "297-th iteration, Loss = 40.309413405079184, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "298-th iteration, Loss = 40.30936623446415, \tTrain acc = 1.0, \tTest acc = 1.0\n",
      "299-th iteration, Loss = 40.30931962817257, \tTrain acc = 1.0, \tTest acc = 1.0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Binary classification\n",
    "# Input dataset: dataset-1.csv\n",
    "# \n",
    "# Princess Tara Zamani\n",
    "# U1139219\n",
    "from numpy import *\n",
    "from numpy.random import randn\n",
    "\n",
    "grad_track = 0\n",
    "\n",
    "# Inputs\n",
    "#   w1, w2 : weights to be updated\n",
    "#   x      : input used for backprop\n",
    "#   y      : actual labels\n",
    "# Outputs\n",
    "#   loss        : binary cross entropy loss\n",
    "#   loss_grad   : binary cross entropy loss gradient\n",
    "#   y_pred      : predicted output\n",
    "#   grad_y_pred : predicted output gradient\n",
    "#   matMult     : input layer output\n",
    "def forward(w1, w2, x, y):\n",
    "    random.seed(0)\n",
    "    matMult = x.dot(w1)\n",
    "    y_pred = sigmoid_activation(matMult.dot(w2)) \n",
    "    delta = y_pred - y\n",
    "    grad_y_pred = 2*delta\n",
    "    b_cE = binary_crossEntropy(y, y_pred)\n",
    "    loss = b_cE[0] \n",
    "    loss_grad = b_cE[1]\n",
    "   \n",
    "    return loss, loss_grad, y_pred, grad_y_pred, matMult\n",
    "\n",
    "\n",
    "# Inputs\n",
    "#   w1, w2      : weights to be updated\n",
    "#   hidden      : hidden (forward value) used for backprop\n",
    "#   x           : input used for backprop\n",
    "#   loss_grad   : loss function gradient\n",
    "#   y_pred      : predicted output\n",
    "#   grad_y_pred : output gradients\n",
    "#   lr          : learning rate\n",
    "# Outputs\n",
    "#   w1, w2      : updated weights\n",
    "def backward(w1,w2,hidden,x,loss_grad,y_pred,grad_y_pred,lr=1e-4):\n",
    "    if not grad_y_pred.all():\n",
    "        return w1,w2\n",
    "    else:\n",
    "        #grad_upstream = grad * (grad_y_pred * (1 - grad_y_pred))\n",
    "        grad_upstream = loss_grad * (y_pred * (1 - y_pred))\n",
    "        grad_w2 = hidden.T.dot(grad_upstream) \n",
    "        grad_hidden = grad_upstream.dot(w2.T) \n",
    "        grad_w1 = x.T.dot(grad_hidden) \n",
    "        w1 -= lr* grad_w1\n",
    "        w2 -= lr* grad_w2\n",
    "        return w1,w2\n",
    "    \n",
    "\n",
    "# Inputs\n",
    "#   w1, w2      : weights to be updated\n",
    "#   x           : input used for backprop\n",
    "#   y           : actual labels\n",
    "#   sample_size : size of the sample\n",
    "# Outputs\n",
    "#   loss     : evaluated loss\n",
    "#   accuracy : accuracy compared to expected outputs\n",
    "def evaluation(w1,w2,x,y,sample_size):\n",
    "    hidden_linear = matmul(x,w1)\n",
    "    y_pred, output = sigmoid_activation(hidden_linear.dot(w2)), sigmoid_activation(hidden_linear.dot(w2))     #  output = linear(w2*h)\n",
    "    delta = y_pred - y        \n",
    "    grad_y_pred = 2*delta\n",
    "    loss = binary_crossEntropy(y, y_pred)   \n",
    "\n",
    "    right_count = 0\n",
    "    y_pred_size = size(y_pred, axis=0)\n",
    "    for idx in range(sample_size):\n",
    "        if (y_pred[idx] > 0.5):\n",
    "            output[idx] = 1\n",
    "        else:\n",
    "            output[idx] = 0\n",
    "\n",
    "        if (output[idx] == y[idx]):\n",
    "            right_count = right_count + 1\n",
    "\n",
    "    accuracy =  right_count / sample_size\n",
    "\n",
    "    return loss[0], accuracy\n",
    "\n",
    "\n",
    "# Inputs\n",
    "#   x : exponent\n",
    "# Outputs\n",
    "#   sigmoid calculation\n",
    "def sigmoid_activation(x):\n",
    "    return 1/(1+exp(-x))\n",
    "\n",
    "\n",
    "# Inputs\n",
    "#   y      : labels of data\n",
    "#   y_pred : predicted output value\n",
    "# Outputs\n",
    "#   loss : Binary Cross-Entropy \n",
    "#   grad : gradient \n",
    "def binary_crossEntropy(y, y_pred):\n",
    "    loss = sum(maximum(y_pred, 0) - y_pred*y + log(1+ exp(- abs(y_pred))))\n",
    "    grad = ((1/(1+exp(- y_pred))) - y.reshape(y.shape[0],1))  # from Z computes the Sigmoid so P_hat - Y, where P_hat = sigma(Z)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data = genfromtxt('dataset-1.csv', delimiter=',')\n",
    "    random.shuffle(data)\n",
    "\n",
    "    sample, D_in, D_out, n_neurons = 80, 4, 1, 32 \n",
    "\n",
    "    x = data[0:sample,0:4]\n",
    "    y = data[0:sample,4:5]\n",
    "\n",
    "    test_x = data[sample:,0:4]\n",
    "    test_y = data[sample:,4:5]\n",
    "\n",
    "    w1, w2 = randn(D_in, n_neurons), randn(n_neurons, D_out)\n",
    "\n",
    "    batched = 1  # batched = 1 => each epoch uses whole training set\n",
    "                 # batched = 0 => 50% in one epoch\n",
    "   \n",
    "    for i in range(300):  # loops 300 epochs \n",
    "        if batched == 1:\n",
    "            loss, loss_grad, y_pred, grad_y_pred, hidden = forward(w1, w2, x, y) \n",
    "            w1, w2 = backward(w1, w2, hidden, x, loss_grad, y_pred, grad_y_pred, lr=5e-5) \n",
    "            eval_result = evaluation(w1, w2, x, y, sample)    # sample = sample size is 80\n",
    "            test_acc = evaluation(w1, w2, test_x, test_y, 20) # sample size is 20\n",
    "            print(f\"{i}-th iteration, Loss = {loss}, \\tTrain acc = {eval_result[1]}, \\tTest acc = {test_acc[1]}\")\n",
    "        else:\n",
    "            x_first_half = x[0:40]\n",
    "            y_first_half = y[0:40]\n",
    "            loss, loss_grad, grad_y_pred, hidden = forward(w1, w2, x_first_half, y_first_half)\n",
    "            w1, w2 = backward(w1, w2, hidden, x_first_half, loss_grad, grad_y_pred, lr=1e-4)\n",
    "\n",
    "            x_second_half = x[40:]\n",
    "            y_second_half = y[40:]\n",
    "            loss, loss_grad, grad_y_pred, hidden = forward(w1, w2, x_second_half, y_second_half)\n",
    "            w1, w2 = backward(w1, w2, hidden, x_second_half, loss_grad, grad_y_pred, lr=1e-4)\n",
    "            eval_result = evaluation(w1, w2, x, y, sample)\n",
    "            print(f\"{i}-th iteration, b_cE = {eval_result[0]},\\t Loss = {loss},\\t Accuracy = {eval_result[1]}\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    sys.exit(main(sys.argv))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
